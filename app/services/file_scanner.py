import os
import logging
import humanize
from datetime import datetime, timedelta
from typing import List, Set, Tuple, Optional, Dict, Any

from app.utils.config import get_nfs_path, get_ext_tag_map, get_backup_days, get_file_categories, upload_stats
from app.utils.file_utils import get_file_modification_time, is_file_in_time_range, normalize_s3_key

class FileScanner:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –±—ç–∫–∞–ø–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def scan_backup_files(
        self, 
        existing_s3_files: Set[str] = None, 
        categories: Optional[List[str]] = None,
        user_id: Optional[int] = None,
        config_id: Optional[int] = None,
        file_extensions: Optional[List[str]] = None,
        min_size: Optional[int] = None,
        max_size: Optional[int] = None,
        skip_time_filter: bool = False,
        backup_days: Optional[int] = None,
        source_directory: Optional[str] = None  # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    ) -> List[Tuple]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –±—ç–∫–∞–ø–æ–≤ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
        
        Args:
            existing_s3_files: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤ S3
            categories: –°–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            file_extensions: –°–ø–∏—Å–æ–∫ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, ['.vbk', '.vib', '.txt'])
            min_size: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
            max_size: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –≤ –±–∞–π—Ç–∞—Ö
            skip_time_filter: –ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –≤—Ä–µ–º–µ–Ω–∏
            backup_days: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
        """
        if existing_s3_files is None:
            existing_s3_files = set()
        
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π config_id
        from app.utils.config import get_nfs_path, get_ext_tag_map, get_backup_days, get_file_categories, get_config
        import os
        config = get_config(user_id=user_id, config_id=config_id)
        nfs_path = config.get('NFS_PATH', '/mnt/backups')
        ext_tag_map = config.get('EXT_TAG_MAP', {})
        if not ext_tag_map:
            ext_tag_map = {
                '.vbk': 'full',
                '.vib': 'incremental',
                '.vbm': 'metadata',
                '.log': 'logs'
            }
        
        if backup_days is None:
            backup_days = int(config.get('BACKUP_DAYS', 7))
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º –µ—ë –∫ –±–∞–∑–æ–≤–æ–º—É –ø—É—Ç–∏
        if source_directory:
            source_directory = source_directory.strip().strip('/')
            if source_directory:
                scan_path = os.path.join(nfs_path, source_directory)
                self.logger.info(f"üìÅ Using source directory: {source_directory} (full path: {scan_path})")
            else:
                scan_path = nfs_path
        else:
            scan_path = nfs_path
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        self.logger.info(f"üîß FileScanner config - NFS_PATH: {nfs_path}, BACKUP_DAYS: {backup_days}, config_id: {config_id}")
        if source_directory:
            self.logger.info(f"üìÅ Source directory: {source_directory}")
        
        if not os.path.exists(scan_path):
            self.logger.error(f"‚ùå Scan path does not exist: {scan_path}")
            return []
        
        self.logger.info(f"üìÇ Scanning directory: {scan_path}")
        
        # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –≤–º–µ—Å—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π
        if file_extensions:
            self.logger.info(f"üìã Filtering by extensions: {file_extensions}")
            # –°–æ–∑–¥–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π ext_tag_map –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π
            extended_ext_tag_map = {}
            for ext in file_extensions:
                ext = ext.lower()
                if not ext.startswith('.'):
                    ext = '.' + ext
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç–µ–≥ –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º "custom"
                tag = ext_tag_map.get(ext, 'custom')
                extended_ext_tag_map[ext] = tag
            ext_tag_map = extended_ext_tag_map
        else:
            if skip_time_filter:
                self.logger.info("‚è±Ô∏è Time filter disabled")
            else:
                self.logger.info(f"‚è±Ô∏è Filter: last {backup_days} days")
        
        selected_categories = categories or get_file_categories(user_id=user_id, config_id=config_id)
        
        # –ü–µ—Ä–µ–¥–∞–µ–º scan_path (–∫–æ—Ç–æ—Ä—ã–π –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π) –≤ _scan_directory
        # –Ω–æ base_path –æ—Å—Ç–∞–µ—Ç—Å—è nfs_path –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
        return self._scan_directory(
            scan_path,  # –°–∫–∞–Ω–∏—Ä—É–µ–º —ç—Ç—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            nfs_path,   # –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
            ext_tag_map, 
            backup_days, 
            existing_s3_files, 
            selected_categories,
            file_extensions=file_extensions,
            min_size=min_size,
            max_size=max_size,
            skip_time_filter=skip_time_filter
        )
    
    def scan_specific_files(
        self,
        file_paths: List[str],
        existing_s3_files: Set[str] = None,
        user_id: Optional[int] = None
    ) -> List[Tuple]:
        """
        –°–∫–∞–Ω–∏—Ä—É–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã –ø–æ –∏—Ö –ø—É—Ç—è–º
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ NFS_PATH)
            existing_s3_files: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –≤ S3
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (full_path, rel_path, tag, file_size)
        """
        if existing_s3_files is None:
            existing_s3_files = set()
        
        from app.utils.config import get_nfs_path, get_ext_tag_map
        nfs_path = get_nfs_path(user_id=user_id)
        ext_tag_map = get_ext_tag_map(user_id=user_id)
        
        if not os.path.exists(nfs_path):
            self.logger.error(f"‚ùå NFS path does not exist: {nfs_path}")
            return []
        
        backup_files = []
        total_size = 0
        
        for rel_path in file_paths:
            full_path = os.path.join(nfs_path, rel_path)
            
            if not os.path.exists(full_path):
                self.logger.warning(f"‚ö†Ô∏è File not found: {full_path}")
                continue
            
            if not os.path.isfile(full_path):
                self.logger.warning(f"‚ö†Ô∏è Not a file: {full_path}")
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —É–∂–µ –≤ S3
            if rel_path in existing_s3_files:
                self.logger.debug(f"‚è≠Ô∏è Skipping existing file: {rel_path}")
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–≥ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
            ext = os.path.splitext(os.path.basename(rel_path))[1].lower()
            tag = ext_tag_map.get(ext, 'custom')
            
            try:
                file_size = os.path.getsize(full_path)
                backup_files.append((full_path, rel_path, tag, file_size))
                total_size += file_size
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Could not process file {rel_path}: {e}")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        upload_stats.total_files = len(backup_files)
        upload_stats.total_bytes = total_size
        
        self.logger.info(f"‚úÖ Scanned {len(backup_files)} specific files, total size: {humanize.naturalsize(total_size)}")
        
        return backup_files
    
    def _scan_directory(
        self, 
        scan_path: str,  # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–µ–π)
        base_path: str,  # –ë–∞–∑–æ–≤–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
        ext_tag_map: dict, 
        backup_days: int, 
        existing_s3_files: Set[str], 
        categories: List[str],
        file_extensions: Optional[List[str]] = None,
        min_size: Optional[int] = None,
        max_size: Optional[int] = None,
        skip_time_filter: bool = False
    ) -> List[Tuple]:
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
        backup_files = []
        total_size = 0
        skipped_time = 0
        skipped_existing = 0
        skipped_size = 0
        
        try:
            for root, dirs, files in os.walk(scan_path):
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                if not upload_stats.is_running:
                    self.logger.info("‚èπÔ∏è Scanning interrupted by user")
                    break
                
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–∫—Ä—ã—Ç—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                dirs[:] = [d for d in dirs if not d.startswith('.')]
                
                for filename in files:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–ª–∞–≥–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
                    if not upload_stats.is_running:
                        self.logger.info("‚èπÔ∏è Scanning interrupted by user")
                        break
                    
                    # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Ñ–∞–π–ª—ã
                    if filename.startswith('.'):
                        continue
                    
                    file_result = self._process_file(
                        root, filename, ext_tag_map, backup_days, 
                        existing_s3_files, base_path, categories,
                        file_extensions=file_extensions,
                        min_size=min_size,
                        max_size=max_size,
                        skip_time_filter=skip_time_filter
                    )
                    
                    if file_result:
                        if file_result == 'skipped_time':
                            skipped_time += 1
                        elif file_result == 'skipped_existing':
                            skipped_existing += 1
                        elif file_result == 'skipped_size':
                            skipped_size += 1
                        else:
                            backup_files.append(file_result)
                            total_size += file_result[3]  # size is at index 3
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            self._update_stats(len(backup_files), total_size, skipped_existing, skipped_time)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            self._log_scan_results(backup_files, skipped_time, skipped_existing, skipped_size, total_size)
            
            return backup_files
            
        except Exception as e:
            self.logger.error(f"‚ùå Error scanning NFS directory: {e}")
            return []
    
    def _process_file(
        self, 
        root: str, 
        filename: str, 
        ext_tag_map: dict, 
        backup_days: int, 
        existing_s3_files: Set[str], 
        nfs_path: str,
        categories: List[str],
        file_extensions: Optional[List[str]] = None,
        min_size: Optional[int] = None,
        max_size: Optional[int] = None,
        skip_time_filter: bool = False
    ):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏"""
        try:
            full_path = os.path.join(root, filename)
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            ext = os.path.splitext(filename)[1].lower()
            
            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –Ω–∏–º
            if file_extensions:
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (–¥–æ–±–∞–≤–ª—è–µ–º —Ç–æ—á–∫—É –µ—Å–ª–∏ –Ω–µ—Ç)
                normalized_exts = [e if e.startswith('.') else '.' + e.lower() for e in file_extensions]
                if ext not in normalized_exts:
                    return None
                tag = ext_tag_map.get(ext, 'custom')
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –ª–æ–≥–∏–∫—É
                tag = ext_tag_map.get(ext)
                if not tag:
                    return None
            
            # –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã)
            if categories and tag not in categories:
                return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω (–µ—Å–ª–∏ –Ω–µ –æ—Ç–∫–ª—é—á–µ–Ω)
            if not skip_time_filter:
                if not is_file_in_time_range(full_path, backup_days):
                    return 'skipped_time'
            
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å
            rel_path = os.path.relpath(full_path, nfs_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª —É–∂–µ –≤ S3
            if rel_path in existing_s3_files:
                return 'skipped_existing'
            
            # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = os.path.getsize(full_path)
            
            # –§–∏–ª—å—Ç—Ä –ø–æ —Ä–∞–∑–º–µ—Ä—É
            if min_size is not None and file_size < min_size:
                return 'skipped_size'
            if max_size is not None and file_size > max_size:
                return 'skipped_size'
            
            return (full_path, rel_path, tag, file_size)
            
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Could not process file {filename}: {e}")
            return None
    
    def _update_stats(self, files_count: int, total_size: int, skipped_existing: int, skipped_time: int):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
        upload_stats.total_files = files_count
        upload_stats.total_bytes = total_size
        upload_stats.skipped_existing = skipped_existing
        upload_stats.skipped_time = skipped_time
    
    def _log_scan_results(
        self, 
        backup_files: List[Tuple], 
        skipped_time: int, 
        skipped_existing: int,
        skipped_size: int,
        total_size: int
    ):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.logger.info(f"üìä Scan results: {len(backup_files)} files to upload")
        if skipped_time > 0:
            self.logger.info(f"‚è≠Ô∏è Skipped {skipped_time} files (outside time range)")
        if skipped_existing > 0:
            self.logger.info(f"‚è≠Ô∏è Skipped {skipped_existing} files (already in S3)")
        if skipped_size > 0:
            self.logger.info(f"‚è≠Ô∏è Skipped {skipped_size} files (size filter)")
        self.logger.info(f"üì¶ Total size to upload: {humanize.naturalsize(total_size)}")
        
        if backup_files:
            large_files = sorted(backup_files, key=lambda x: x[3], reverse=True)[:5]
            self.logger.info("üìã Top 5 largest files to upload:")
            for full, rel, tag, size in large_files:
                file_time = get_file_modification_time(full)
                self.logger.info(f"  {humanize.naturalsize(size):>10} - {file_time.strftime('%Y-%m-%d %H:%M')} - {rel}")

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
file_scanner = FileScanner()

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
def scan_backup_files(
    existing_s3_files=None, 
    categories: Optional[List[str]] = None,
    user_id: Optional[int] = None,
    config_id: Optional[int] = None,
    file_extensions: Optional[List[str]] = None,
    min_size: Optional[int] = None,
    max_size: Optional[int] = None,
    skip_time_filter: bool = False,
    backup_days: Optional[int] = None,
    source_directory: Optional[str] = None
):
    return file_scanner.scan_backup_files(
        existing_s3_files, 
        categories, 
        user_id,
        config_id,
        file_extensions,
        min_size,
        max_size,
        skip_time_filter,
        backup_days,
        source_directory
    )

def scan_specific_files(
    file_paths: List[str],
    existing_s3_files: Optional[Set[str]] = None,
    user_id: Optional[int] = None
):
    """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
    return file_scanner.scan_specific_files(file_paths, existing_s3_files, user_id)

def get_file_modification_time(file_path):
    from app.utils.file_utils import get_file_modification_time as get_mtime
    return get_mtime(file_path)

def normalize_s3_key(tag: str, rel_path: str) -> str:
    from app.utils.file_utils import normalize_s3_key as normalize_key
    return normalize_key(tag, rel_path)

def is_file_in_time_range(file_path, days_back):
    from app.utils.file_utils import is_file_in_time_range as in_time_range
    return in_time_range(file_path, days_back)
